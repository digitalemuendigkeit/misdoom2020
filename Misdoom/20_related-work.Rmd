# Deliberation in Digital Media
Since the commercialization of the internet, the relationship between digital media and political life has grown ever stronger [@boulianneTwentyYearsDigital2018]. 
Apart from election campaigns, one important facet is the internet's potential to strengthen democratic society by facilitating public deliberation [@papacharissiVirtualSphereInternet2002; @chenOnlineIncivilityPublic2017].
At the same time, several limitations for online public deliberation have emerged. 
For one, offline power imbalances are often mirrored in the online world through an overrepresentation of groups in power, e.g., well-educated white men [@feezellPredictingOnlinePolitical2016]. In addition, online groups tend to be very homogenous meaning that users are seldomly exposed to cross-cutting opinions or differing viewpoints [@colleoniEchoChamberPublic2014]. And when differing opinions do collide, incivility and even hate speech can occur [@papacharissiDemocracyOnlineCivility2004; @coeOnlineUncivilPatterns2014; @rainsIncivilityPoliticalIdentity2017].

## Hate Speech and Misinformation
Although hate speech has been extensively discussed by the public at large as well as studied in academia, finding an universally valid definition is challenging [@gagliardoneCounteringOnlineHate2015]. Legal institutions and social networks alike tend to provide broad definitions that allow for judgement and possible sanctions on a case-by-case basis [@gagliardoneCounteringOnlineHate2015].

From a communication science perspective, @erjavecYouDonUnderstand2012 define hate speech as an expression that is in itself harmful or possibly harm-inciting and targets members of a group determined by characteristics like race or sexual orientation. Similar characteristics can be found in other definitions that consider the purpose and the effects of hate speech. @waldronHarmHateSpeech2012 characterizes speech as hateful when it serves one or both of two functions: Firstly, to dehumanize a target group and diminish its members and secondly, to reinforce a sense of in-group with other like-minded individuals. Similarly, Susan Benesch has coined the term _dangerous speech_ which she defines as "[a]ny form of expression (e.g. speech, text, or images) that can increase the risk that its audience will condone or commit violence against members of another group." [@beneschDangerousSpeechPractical2020] Often, the groups targeted are marginalized social groups [@alvarez-benjumeaNormativeChangeCulture2018]. But especially in common parlance, hate speech can also describe speech directed at groups like politicians that are arguably powerful [@gagliardoneCounteringOnlineHate2015]. In summary, hate speech both reenforces the boundaries between groups and is harmful to members of the other group, either in itself or in its effects.

Hate speech is inextricably linked to disinformation. For one, online hate often takes the form of over-generalization, exaggeration or even outright deceit about the targeted group. E.g., @awanIslamophobiaSocialMedia2016 stresses the use of false stories to exacerbate islamophobic hate. For another, disinformation like fake news often serve the same purpose as hate speech: Polarization, radicalization and othering of the out-group [@bennettDisinformationOrderDisruptive2018].

There is ample evidence for the damaging effects of hate speech, not only on the victim of the hate speech but also on the broader audience. 
Constant exposition shapes the users worldview and influences their decision-making [@jubanyBackgroundsExperiencesResponses2016; @erjavecReadersOnlineNews2014]. 
Reading hateful and uncivil content increases attitude polarization [@borahDoesItMatter2014; @kimIncivilityFacebookPolitical2019].
And by inducing negative emotions, it can also discourage people from engaging in discourse [@hwangDoesCivilityMatter22; @hwangSeeingBelievingEffects2014; @kimIncivilityFacebookPolitical2019; @molinaRoleCivilityMetacommunication2018]. Thereby, it actively impedes on the internet's potential for public deliberation.

One possible way to counter hate speech is counter speech. Counter speech can be defined as a dissenting response to hate speech [@wrightVectorsCounterspeechTwitter2017]. 
Although it is sometimes used in a way that also encompasses actions like flagging hateful content, our study focusses on counter speech in the form of content, e.g., comments in answer to the hateful content itself.

## Effectiveness of Counter Speech
Counter speaking is encouraged in many anti-hate speech programs [@gagliardoneCounteringOnlineHate2015]. Furthermore, @chenOnlineIncivilityPublic2017 argues that countering online incivility is necessary to realize the potential of online spaces as a place for political deliberation. In spite of that, only a few studies have actually evaluated the effectiveness of counter speech. @buergerCounterspeechLiteratureReview2019 have reviewed the studies available in November 2019. They differentiate between the effects of counter speech on the hateful speaker and the effects on the wider audience.

The results oncerning the effects of counter speech on the hateful speaker are inconclusive [@buergerCounterspeechLiteratureReview2019]. However, there is some evidence that counter speech by users that are perceived as more influential, can curb hate speech at least temporarily (e.g., [@mungerTweetmentEffectsTweeted2017]). Findings on the effects of counter speech on the wider audience are less ambiguous. They all find evidence for something that @buergerCounterspeechLiteratureReview2019 call the "contagion effect", i.e., the presence of hateful comments increases the probability of a user also making a hateful comment. On the opposite hand, civil comments also lead to more civil comments. Moreover, meta-comments urging people to be civil promote further meta-comments about discussion quality [@molinaRoleCivilityMetacommunication2018].

So while further research on the effects of counter speech is desirable, the existing indications for its success prompts us to ask what predicts users engaging in counter speech.

## Predictors for Counter Speech
There already exist several studies examining willingness to intervene against hate speech and incivility in general and even more studies from the field of cyber-bystander research. As bystander intervention in cyber-bullying is similar to counter speech, predictors from a review on cyber-bystanding  studies by @lambeStandingBullyingSocial2019 are included as well.

The following predictors refer to intervention intention, with intervention ranging from more distanced behavior like using the reporting function (e.g., [@wongCombatingOnlineAbuse2016]) to deeply involved behavior like verbally confronting the individual engaging in hate speech (e.g., [@dickterConfrontNotConfront2013]).

The factors we summarize as _individual factors_ concern properties of a would-be counter speaker that make intervention more likely. The predictors found are female gender [@lambeStandingBullyingSocial2019; @wilhelmGenderedMoralityBacklash2019], high prosociality and empathy [@lambeStandingBullyingSocial2019], high self-efficacy [@lambeStandingBullyingSocial2019], a negative attitude towards passive bystanding [@lambeStandingBullyingSocial2019], an expectation that defending will help [@lambeStandingBullyingSocial2019], and a high importance of morality, including low moral disengagement, high moral identity scores and individualizing moral foundation [@lambeStandingBullyingSocial2019; @wilhelmGenderedMoralityBacklash2019]. Additionally, there are situation-dependent factors like the would-be counter speaker feeling negative affect [@chenOnlineIncivilityPublic2017; @dickterConfrontingHateHeterosexuals2012; @dickterConfrontNotConfront2013] and them perceiving social pressure and responsibility to intervene [@dickterConfrontingHateHeterosexuals2012; @dickterConfrontNotConfront2013; @wongCombatingOnlineAbuse2016].

Other factors relate to the _properties of the victim_ of the hate speech or bullying. Users are more likely to intervene if the victim is an individual person as a victim rather than an abstract social group  [@naabFlaggingUncivilUser2018], if the victim is more popular [@lambeStandingBullyingSocial2019], if they have a friendship or positive relationship with the victim [@lambeStandingBullyingSocial2019] and if they exhibit a low level of prejudice towards the victims's social group. [@dickterConfrontingHateHeterosexuals2012]
    
Concerning _situational factors_, users were more likely to intervene if the situation was more deviant [@dickterConfrontingHateHeterosexuals2012; @dickterConfrontNotConfront2013; @naabFlaggingUncivilUser2018], if there was  more than one perpetrator [@kazerooniCyberbullyingBystanderIntervention2018] and if more steps of the bystander intervention model were met (situation is noticed, fewer number of bystanders, information on how to confront is provided) [@lambeStandingBullyingSocial2019; @naabFlaggingUncivilUser2018].

To summarize, the existing research on hate speech intervention mainly considers the properties of the would-be counter speaker. For this study, we wanted to further the research on hate speech intervention by focussing in on the relationships between the would-be counter speaker and the victim. To be precise, with hate speech as an instrument for social division and polarization, can counter\ speech bridge the gap between in- and out-group? Therefore, our research question is:

*In social media discussions, what are predictors for users to engage in counter speech in support of political adversaries?*